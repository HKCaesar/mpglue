{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0B3B2E;float:right;font-family:Calibri\">Jordan Graesser</span>\n",
    "\n",
    "# MpGlue\n",
    "### Preparing data and testing model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Ranking and optimizing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpglue as gl\n",
    "\n",
    "CL = gl.classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method rank_feas in module mpglue.classification.classification:\n",
      "\n",
      "rank_feas(self, rank_text=None, rank_method='chi2', top_feas=1.0, be_quiet=False) method of mpglue.classification.classification.classification instance\n",
      "    Ranks image features by importance.\n",
      "    \n",
      "    Args:\n",
      "        rank_text (Optional[str]): A text file to write ranked features to. Default is None.\n",
      "        rank_method (Optional[str]): The method to use for feature ranking. Default is 'chi2' (Chi^2). Choices are \n",
      "            ['chi2', 'RF'].\n",
      "        top_feas (Optional[float or int]): The percentage or total number of features to reduce to. \n",
      "            Default is 1., or no reduction.\n",
      "        be_quiet (Optional[bool]): Whether to be quiet and do not print to screen. Default is False.\n",
      "    \n",
      "    Returns:\n",
      "        None, writes to ``rank_text`` if given and prints results to screen.\n",
      "    \n",
      "    Examples:\n",
      "        >>> # rank image features\n",
      "        >>> cl.split_samples('/samples.txt', scale_data=True)\n",
      "        >>> cl.rank_feas(rank_text='/ranked_feas.txt',\n",
      "        >>>              rank_method='chi2', top_feas=.2)\n",
      "        >>> print cl.fea_rank\n",
      "        >>>\n",
      "        >>> # a RF model must be trained before feature ranking\n",
      "        >>> cl.construct_model()\n",
      "        >>> cl.rank_feas(rank_method='RF', top_feas=.5)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print help documentation for the Chi-square \n",
    "#   test to rank feature importance.\n",
    "print help(CL.rank_feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = '../testing/data/08N_points_merged.txt'\n",
    "\n",
    "# Sample the data\n",
    "CL.split_samples(samples)\n",
    "\n",
    "# Rank the explanatory variables with a chi-square test\n",
    "CL.rank_feas(rank_method='chi2', top_feas=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:37:01:INFO:4340:classification._train_model:  Training a RF model with 4,783 samples and 41 variables ...\n"
     ]
    }
   ],
   "source": [
    "# Rank the explanatory variables with a Random Forest \n",
    "#   model and compare results.\n",
    "\n",
    "# First, construct a Random Forest model.\n",
    "CL.construct_model(classifier_info={'classifier': 'RF'})\n",
    "\n",
    "# Use the RF model to rank feature importance.\n",
    "CL.rank_feas(rank_method='RF', top_feas=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimizing parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method optimize_parameters in module mpglue.classification.classification:\n",
      "\n",
      "optimize_parameters(self, file_name, classifier_info={'classifier': 'RF'}, n_trees_list=[500, 1000, 1500, 2000], trials_list=[2, 5, 10], max_depth_list=[25, 30, 35, 40, 45, 50], min_samps_list=[2, 5, 10], criterion_list=['gini'], rand_vars_list=['sqrt'], cf_list=[0.25, 0.5, 0.75], committees_list=[1, 2, 5, 10], rules_list=[25, 50, 100, 500], extrapolation_list=[0, 1, 5, 10], class_weight_list=[None, 'balanced', 'balanced_subsample'], learn_rate_list=[0.1, 0.2, 0.4, 0.6, 0.8, 1.0], bool_list=[True, False], c_list=[1.0, 10.0, 20.0, 100.0], gamma_list=[0.001, 0.001, 0.01, 0.1, 1.0, 5.0], k_folds=3, perc_samp=0.5, ignore_feas=[], use_xy=False, classes2remove=[], method='overall', f1_class=0, stratified=False, spacing=1000.0, calibrate_proba=False, output_file=None) method of mpglue.classification.classification.classification instance\n",
      "    Finds the optimal parameters for a classifier by training and testing a range of classifier parameters\n",
      "    by n-folds cross-validation.\n",
      "    \n",
      "    Args:\n",
      "        file_name (str): The file name of the samples.\n",
      "        classifier_info (Optional[dict]): The model parameters dictionary. Default is {'classifier': 'RF'}.\n",
      "        n_trees_list (Optional[int list]): A list of trees. Default is [500, 1000].\n",
      "        trials_list (Optional[int list]): A list of boosting trials. Default is [5, 10, 20].\n",
      "        max_depth_list (Optional[int list]): A list of maximum depths. Default is [5, 10, 20, 25, 30, 50].\n",
      "        min_samps_list (Optional[int list]): A list of minimum samples. Default is [2, 5, 10].\n",
      "        criterion_list (Optional[str list]): A list of RF criterion. Default is ['gini', 'entropy'].\n",
      "        rand_vars_list (Optional[str list]): A list of random variables. Default is ['sqrt'].\n",
      "        class_weight_list (Optional[bool]): A list of class weights.\n",
      "            Default is [None, 'balanced', 'balanced_subsample'].\n",
      "        c_list (Optional[float list]): A list of SVM C parameters. Default is [1., 10., 20., 100.].\n",
      "        gamma_list (Optional[float list]): A list of SVM gamma parameters. Default is [.001, .001, .01, .1, 1., 5.].\n",
      "        k_folds (Optional[int]): The number of N cross-validation folds. Default is 3.\n",
      "        ignore_feas (Optional[int list]): A list of features to ignore. Default is [].\n",
      "        use_xy (Optional[bool]): Whether to use x, y coordinates. Default is False.\n",
      "        classes2remove (Optional[int list]): A list of classes to remove. Default is [].\n",
      "        method (Optional[str]): The score method to use, 'overall' (default) or 'f1'.1\n",
      "        f1_class (Optional[int]): The class position to evaluate when ``method`` is equal to 'f1'. Default is 0,\n",
      "            or first index position.\n",
      "        stratified (Optional[bool]):\n",
      "        spacing (Optional[float]):\n",
      "        output_file (Optional[str]):\n",
      "    \n",
      "    Returns:\n",
      "        `Pandas DataFrame` when classifier_info['classifier'] == 'C5',\n",
      "            otherwise None, prints results to screen.\n",
      "    \n",
      "    Examples:\n",
      "        >>> # find the optimal parameters (max depth, min samps, trees)\n",
      "        >>> # randomly sampling 50% (with replacement) and testing on the 50% set aside\n",
      "        >>> # repeat 5 (k_folds) times and average the results\n",
      "        >>> from mappy.classifiers import classification\n",
      "        >>> cl = classification()\n",
      "        >>>\n",
      "        >>> # Find the optimum parameters for an Extremely Randomized Forest.\n",
      "        >>> cl.optimize_parameters('/samples.txt',\n",
      "        >>>                        classifier_info={'classifier': 'EX_RF'},\n",
      "        >>>                        use_xy=True)\n",
      "        >>>\n",
      "        >>> # Find the optimum parameters for a Random Forest, but assess\n",
      "        >>> #   only one class (1st class position) of interest.\n",
      "        >>> cl.optimize_parameters('/samples.txt',\n",
      "        >>>                        classifier_info={'classifier': 'RF'},\n",
      "        >>>                        use_xy=True, method='f1', f1_class=0)\n",
      "        >>>\n",
      "        >>> # Optimizing C5 parameters\n",
      "        >>> from mappy.classifiers import classification_r\n",
      "        >>> cl = classification_r()\n",
      "        >>>\n",
      "        >>> df = cl.optimize_parameters('/samples.txt', classifier_info={'classifier': 'C5'},\n",
      "        >>>                             trials_list=[2, 5, 10], cf_list=[.25, .5, .75],\n",
      "        >>>                             min_samps_list=[2, 5, 10], bool_list=[True, False],\n",
      "        >>>                             k_folds=5, stratified=True, spacing=50000.)\n",
      "        >>>\n",
      "        >>> print df\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print help(CL.optimize_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding the best paramaters for a RF model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum parameters for a Random Forest,\n",
    "#   using the default parameter list.\n",
    "CL.optimize_parameters(samples, \n",
    "                       classifier_info={'classifier': 'RF'}, \n",
    "                       use_xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on error_matrix in module mpglue.classification.error_matrix object:\n",
      "\n",
      "class error_matrix(__builtin__.object)\n",
      " |  Computes accuracy statistics\n",
      " |  \n",
      " |  Args:\n",
      " |      po_text (str): Predicted and observed labels as a text file, where (predicted, observed)\n",
      " |          are the last two columns.\n",
      " |      po_array (ndarray): Predicted and observed labels as an array, where (predicted, observed)\n",
      " |          are the last two columns.\n",
      " |      header (Optional[bool]): Whether ``file`` or ``predicted_observed`` contains a header. Default is False.\n",
      " |      class_list (Optional[list])\n",
      " |      discrete (Optional[bool])\n",
      " |      e_matrix (Optional[ndarray])\n",
      " |  \n",
      " |  Attributes:\n",
      " |      n_classes (int): Number of unique classes.\n",
      " |      class_list (list): List of unique classes.\n",
      " |      e_matrix (ndarray): Error matrix.\n",
      " |      accuracy (float): Overall accuracy.\n",
      " |      report\n",
      " |      f_scores (float)\n",
      " |      f_beta (float)\n",
      " |      hamming (float)\n",
      " |      kappa_score (float)\n",
      " |      mae (float)\n",
      " |      mse (float)\n",
      " |      rmse (float)\n",
      " |      r_squared (float)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> from mappy.sample import error_matrix\n",
      " |      >>>\n",
      " |      >>> emat = error_matrix()\n",
      " |      >>>\n",
      " |      >>> # Get an accuracy report from an array\n",
      " |      >>> emat.get_stats(po_array=test_array)\n",
      " |      >>> print emat.accuracy\n",
      " |      >>>\n",
      " |      >>> # Get an accuracy report from a text file\n",
      " |      >>> emat.get_stats(po_text='/test_samples.txt')\n",
      " |      >>>\n",
      " |      >>> # Write statistics to file\n",
      " |      >>> emat.write_stats('/accuracy_report.txt')\n",
      " |  \n",
      " |  Returns:\n",
      " |      None, writes to ``files`` if given.\n",
      " |  \n",
      " |  Reference:\n",
      " |      Overall accuracy:\n",
      " |          where,\n",
      " |              Oacc = D / (N * 100)\n",
      " |                  where,\n",
      " |                      D = total number of correctly labeled samples (on the diagonal)\n",
      " |                      N = total number of samples in the matrix\n",
      " |      Kappa:\n",
      " |          : Measure of agreement\n",
      " |      F1-score:\n",
      " |          where,\n",
      " |              F-measure = 2 * ((Producer's * User's) / (Producer's + User's))\n",
      " |      Producer's accuracy:\n",
      " |          Omission error\n",
      " |              -- \"excluding an area from the category in which it truly belongs\"\n",
      " |                  - Congalton and Green (1999)\n",
      " |  \n",
      " |            # correctly classified observed samples for class N\n",
      " |          = ----------------------------------------------------- x 100\n",
      " |            total # of observed (column-wise) samples for class N\n",
      " |  \n",
      " |      User's accuracy:\n",
      " |          Commission error\n",
      " |              -- \"including an area into a category when it does not belong to that category\"\n",
      " |                  - Congalton and Green (1999)\n",
      " |  \n",
      " |            # correctly classified predicted samples for class N\n",
      " |          = ---------------------------------------------------- x 100\n",
      " |            total # of predicted (row-wise) samples for class N\n",
      " |  \n",
      " |      RMSE:\n",
      " |          where,\n",
      " |              (Square root of (Sum of (x - y)^2) / N)\n",
      " |  \n",
      " |      |========================================================================|\n",
      " |      |                ********************                                    |\n",
      " |      |                * Confusion Matrix *                                    |\n",
      " |      |                ********************                                    |\n",
      " |      |                                                                        |\n",
      " |      |                   Observed/                                            |\n",
      " |      |                   Reference                                            |\n",
      " |      |                  ---------------                                       |\n",
      " |      |                  C0   C1   C2   ..   Cn  | Column totals | User's (%)  |\n",
      " |      |                  ----------------------- | ------------- | ----------- |\n",
      " |      |  Predicted/| C0 |(#)                     | sum(C0 row)   | %           |\n",
      " |      |  Labeled   | C1 |     (#)                | sum(C1 row)   | %           |\n",
      " |      |            | C2 |          (#)           | sum(C2 row)   | %           |\n",
      " |      |            | .. |               ..       | ..            | %           |\n",
      " |      |            | Cn |                   (#)  | sum(Cn row)   | %           |\n",
      " |      |       Row totals|                        | (TOTAL)       |             |\n",
      " |      |   Producer's (%)| %    %    %   ..   %   |               | (Overall %) |\n",
      " |      |========================================================================|\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |  \n",
      " |  error_matrix2xy(self)\n",
      " |      Reverses the error matrix to predictions and observations\n",
      " |  \n",
      " |  get_stats(self, po_text=None, po_array=None, header=False, class_list=None, discrete=True, e_matrix=None)\n",
      " |  \n",
      " |  kappa(self, y_true, y_pred, weights=None, allow_off_by_one=False)\n",
      " |      Calculates the kappa inter-rater agreement between two the gold standard\n",
      " |      and the predicted ratings. Potential values range from -1 (representing\n",
      " |      complete disagreement) to 1 (representing complete agreement).  A kappa\n",
      " |      value of 0 is expected if all agreement is due to chance.\n",
      " |      \n",
      " |      In the course of calculating kappa, all items in ``y_true`` and ``y_pred`` will\n",
      " |      first be converted to floats and then rounded to integers.\n",
      " |      \n",
      " |      It is assumed that ``y_true`` and ``y_pred`` contain the complete range of possible\n",
      " |      ratings.\n",
      " |      \n",
      " |      This function contains a combination of code from yorchopolis's kappa-stats\n",
      " |      and Ben Hamner's Metrics projects on Github.\n",
      " |      \n",
      " |      Args:\n",
      " |          y_true\n",
      " |          y_pred\n",
      " |          weights (Optional[str or numpy array]): Specifies the weight matrix for the calculation. Choices\n",
      " |              are [None :: unweighted-kappa, 'quadratic' :: quadratic-weighted kappa, 'linear' ::\n",
      " |              linear-weighted kappa, two-dimensional numpy array :: a custom matrix of weights. Each weight\n",
      " |              corresponds to the :math:`w_{ij}` values in the wikipedia description of how to calculate\n",
      " |              weighted Cohen's kappa.]\n",
      " |          allow_off_by_one (Optional[bool]): If true, ratings that are off by one are counted as equal, and\n",
      " |              all other differences are reduced by one. For example, 1 and 2 will be considered to be\n",
      " |              equal, whereas 1 and 3 will have a difference of 1 for when building the weights matrix.\n",
      " |      \n",
      " |      Reference:\n",
      " |          Authors: SciKit-Learn Laboratory\n",
      " |          https://skll.readthedocs.org/en/latest/_modules/skll/metrics.html\n",
      " |  \n",
      " |  merge_lists(self, list1, list2)\n",
      " |  \n",
      " |  producers_accuracy(self)\n",
      " |  \n",
      " |  sample_bias(self, class_area)\n",
      " |      Calculates the area adjusted sampling bias\n",
      " |      \n",
      " |      Args:\n",
      " |          class_area (list): A list of class areas.\n",
      " |      \n",
      " |      References:\n",
      " |          Olofsson et al. (2013) Note that the model assessment on 30% of the withheld\n",
      " |              samples was conducted post-parameter optimization (section 3.4) on the final\n",
      " |              parameter set, and not on cross-validated samples. Remote Sensing of Environment 129, 122-131.\n",
      " |      \n",
      " |      Test:\n",
      " |          emat = error_matrix()\n",
      " |          class_area = [22353, 1122543, 610228]\n",
      " |          emat.e_matrix = np.array([[97, 0, 3],[3, 279, 18], [2, 1, 97]], dtype='float32')\n",
      " |          emat.sample_bias(class_area)\n",
      " |      \n",
      " |          print emat.stratified_estimate\n",
      " |          print emat.standard_errors\n",
      " |  \n",
      " |  users_accuracy(self)\n",
      " |  \n",
      " |  write_stats(self, out_report)\n",
      " |      Args:\n",
      " |          out_report (str): The file to write to.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "emat = gl.error_matrix()\n",
    "\n",
    "print help(emat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create some random data\n",
    "test_array = np.random.randn(100, 2).astype('uint8')\n",
    "\n",
    "emat.get_stats(po_array=test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'accuracy', 'class_list', 'discrete', 'e_matrix', 'error_matrix2xy', 'f_beta', 'f_scores', 'get_stats', 'hamming', 'kappa', 'kappa_score', 'merge_lists', 'n_classes', 'n_samples', 'n_samps', 'producers', 'producers_accuracy', 'report', 'sample_bias', 'time_stamp', 'users', 'users_accuracy', 'write_stats', 'y']\n"
     ]
    }
   ],
   "source": [
    "print dir(emat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n"
     ]
    }
   ],
   "source": [
    "print emat.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  7  1  0 19]\n",
      " [ 9  0  0  1  1]\n",
      " [ 0  1  0  0  1]\n",
      " [ 2  0  0  0  0]\n",
      " [ 7  1  1  0  1]]\n"
     ]
    }
   ],
   "source": [
    "print emat.e_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0793650793651\n"
     ]
    }
   ],
   "source": [
    "print emat.kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.73      0.68        66\n",
      "          1       0.00      0.00      0.00         9\n",
      "          2       0.00      0.00      0.00         2\n",
      "        254       0.00      0.00      0.00         1\n",
      "        255       0.10      0.05      0.06        22\n",
      "\n",
      "avg / total       0.44      0.49      0.46       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print emat.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print emat.write_stats('datasets/my_report.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "### Models in MpGlue\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Supported models\n",
      "\n",
      "        ===========================\n",
      "        Parameter name -- Long name\n",
      "              {Classifier defaults}\n",
      "              *Scikit-learn parameter names and defaults\n",
      "        ===========================\n",
      "\n",
      "        AB_DT -- AdaBoost with CART (classification problems)\n",
      "              *Scikit-learn\n",
      "        AB_EX_DT-- AdaBoost with extremely random trees (classification problems)\n",
      "              *Scikit-learn\n",
      "        AB_RF-- AdaBoost with Random Forest (classification problems)\n",
      "              *Scikit-learn\n",
      "        AB_EX_RF-- AdaBoost with Extremely Random Forest (classification problems)\n",
      "              *Scikit-learn\n",
      "        AB_DTR-- AdaBoost with CART (regression problems)\n",
      "              *Scikit-learn\n",
      "        AB_EX_DTR-- AdaBoost with extremely random trees (regression problems)\n",
      "              *Scikit-learn\n",
      "        Bag   -- Bagging (classification problems)\n",
      "              *Scikit-learn\n",
      "        BagR  -- Bagging (regression problems)\n",
      "              *Scikit-learn\n",
      "        Bag_EX_DT-- Bagging with extra trees (classification problems)\n",
      "              *Scikit-learn\n",
      "        Bayes -- Naives Bayes (classification problems)\n",
      "        DT    -- Decision Trees based on CART algorithm (classification problems)\n",
      "              *Scikit-learn\n",
      "        DTR   -- Decision Trees Regression based on CART algorithm (regression problems)\n",
      "              *Scikit-learn\n",
      "        EX_DT -- Extra Decision Trees based on CART algorithm (classification problems)\n",
      "              *Scikit-learn\n",
      "        EX_DTR-- Extra Decision Trees Regression based on CART algorithm (regression problems)\n",
      "              *Scikit-learn\n",
      "        GB    -- Gradient Boosted Trees (classification problems)\n",
      "              *Scikit-learn\n",
      "        GBR   -- Gradient Boosted Trees (regression problems)\n",
      "              *Scikit-learn\n",
      "        C5    -- C5 decision trees (classification problems)\n",
      "              {classifier:C5,trials:10,CF:.25,min_cases:2,winnow:False,no_prune:False,fuzzy:False}\n",
      "        Cubist-- Cubist regression trees (regression problems)\n",
      "              {classifier:Cubist,committees:5,unbiased:False,rules:100,extrapolation:10}\n",
      "        EX_RF -- Extremely Random Forests (classification problems)\n",
      "              *Scikit-learn\n",
      "        CVEX_RF -- Extremely Random Forests in OpenCV (classification problems)\n",
      "              *NOT CURRENTLY SUPPORTED IN OPENCV 3.0*\n",
      "              {classifier:CVEX_RF,trees:1000,min_samps:0,rand_vars:sqrt(feas),max_depth:25}\n",
      "        EX_RFR-- Extremely Random Forests (regression problems)\n",
      "              *Scikit-learn\n",
      "        Gaussian-- Gaussian Process (classification problems)\n",
      "              *Scikit-learn\n",
      "        Logistic-- Logistic Regression (classification problems)\n",
      "              *Scikit-learn\n",
      "        NN    -- K Nearest Neighbor (classification problems)\n",
      "              *Scikit-learn\n",
      "        RF    -- Random Forests (classification problems)\n",
      "              *Scikit-learn\n",
      "        CVRF  -- Random Forests in OpenCV (classification problems)\n",
      "              {classifier:CVRF,trees:1000,min_samps:0,rand_vars:0,max_depth:25,weight_classes:None,truncate:False}\n",
      "        RFR   -- Random Forests (regression problems)\n",
      "              *Scikit-learn\n",
      "        CVMLP -- Feed-forward, artificial neural network, multi-layer perceptrons in OpenCV (classification problems)\n",
      "              {classifier:CVMLP}\n",
      "        SVM   -- Support Vector Machine (classification problems)\n",
      "              {classifier:SVM,C:1,g:1.}\n",
      "        SVMR  -- Support Vector Machine (regression problems)\n",
      "              {classifier:SVMR,C:1,g:1.}\n",
      "        CVSVM -- Support Vector Machine in OpenCV (classification problems)\n",
      "              {classifier:CVSVM,C:1,g:1.}\n",
      "        CVSVMA-- Support Vector Machine, auto-tuned in OpenCV (classification problems)\n",
      "              {classifier:CVSVMA}\n",
      "        CVSVMR-- Support Vector Machine in OpenCV (regression problems)\n",
      "              {classifier:CVSVMR,C:1,g:1.}\n",
      "        CVSVMRA-- Support Vector Machine, auto-tuned in OpenCV (regression problems)\n",
      "              {classifier:CVSVMRA}\n",
      "        QDA   -- Quadratic Discriminant Analysis (classification problems)\n",
      "              *Scikit-learn\n",
      "        ChainCRF-- Linear-chain Conditional Random Fields (classification problems)\n",
      "              *Pystruct\n",
      "        GridCRF -- Pairwise Conditional Random Fields on a 2d grid (classification problems)\n",
      "              *Pystruct\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Check the available models.\n",
    "print CL.model_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training a RF model with 4,783 samples and 41 variables ...\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Construct a Random Forest model with 100 trees.\n",
    "CL.construct_model(classifier_info={'classifier': 'RF',\n",
    "                                    'n_estimators': 100})\n",
    "\n",
    "# The model is stored in `model`.\n",
    "print CL.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training an EX_RF model with 4,783 samples and 41 variables ...\n",
      "\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Construct an extremely randomized Random Forest \n",
    "#   with 100 trees.\n",
    "CL.construct_model(classifier_info={'classifier': 'EX_RF',\n",
    "                                    'n_estimators': 100})\n",
    "\n",
    "print CL.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training an AB_EX_RF model with 4,783 samples and 41 variables ...\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False),\n",
      "          learning_rate=0.1, n_estimators=10, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# Construct a boosted extremely randomized Random Forest with \n",
    "#   100 trees and 10 trials (boosts).\n",
    "CL.construct_model(classifier_info={'classifier': 'AB_EX_RF',\n",
    "                                    'n_estimators': 100,\n",
    "                                    'trials': 10})\n",
    "\n",
    "print CL.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We trained one model in the examples above.\n",
    "* MpGlue also supports training ensemble models through Scikit-learn's `VotingClassifier` module.\n",
    "* To train a voting classifier, simply provide a list of classifiers instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct an ensemble voting model and \n",
    "#   save the model to file.\n",
    "CL.construct_model(classifier_info={'classifier': ['RF', 'EX_RF', 'Bayes', 'QDA', 'NN'],\n",
    "                                    'n_estimators': 100,\n",
    "                                    'trials': 5},\n",
    "                   output_model='/voting_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Making predictions on an image\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the voting model.\n",
    "CL.construct_model(input_model='/voting_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an image.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif')\n",
    "\n",
    "# *Note that the model could also be loaded with `predict`.\n",
    "# This syntax would not require the `construct_model` method.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif',\n",
    "           input_model='voting_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an image, adjusting the block size.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif',\n",
    "           row_block_size=2048,\n",
    "           col_block_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an image, adjusting the number of parallel jobs.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif',\n",
    "           n_jobs=4,       # model parallel jobs\n",
    "           n_jobs_vars=4)  # image band reading parallel jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an image, and then apply\n",
    "#   posterior probability label relaxation.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif',\n",
    "           relax_probabilities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The default for `predict` is to apply predictions block by block, reading from one image and writing to one image.\n",
    "* However, sometimes the input image might be very large, making block writes to the output slow.\n",
    "* `predict` can write to individual blocks instead of to one image.\n",
    "* In the example below, individual blocks will be written as `/output_image_00001.tif`, `/output_image_00002.tif`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to an image, writing to individual blocks.\n",
    "CL.predict('/input_image.tif',\n",
    "           '/output_image.tif',\n",
    "           write2blocks=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
